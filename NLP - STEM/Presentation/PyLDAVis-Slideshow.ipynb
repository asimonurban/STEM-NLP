{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alpho\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\matplotlib\\colors.py:53: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sized\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "path_link = \"C:/Users/alpho/Dropbox/NLP - STEM/Presentation\"\n",
    "os.chdir(path_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Utlizing Natural Language Processing in Social Science Research: Topic Modeling\n",
    "###                                            By Daniel Kuehn and Alphonse Simon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![LDA Example](ml_nlp.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![LDA Example](ist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![LDA Example](ist_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![LDA Example](gender.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![LDA Example](gender_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![LDA Example](topic_modeling.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Objective\n",
    "\n",
    "In this slideshow and the accompanying blog posts (not-urban affilated), I aim to answer the following questions: \n",
    "\n",
    "1. What is the motivation for learning Topic Modeling?\n",
    "2. What does Topic Modeling actually do?\n",
    "3. How can Topic Modeling be used in Social Science research? \n",
    "4. How do I get to the point where I can trust and interpret the model?\n",
    "5. How can I most efficiently access the tool?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Objective\n",
    "\n",
    "In this slideshow and the accompanying blog posts (not-urban affilated), I aim to answer the following questions: \n",
    "\n",
    "1. What is the motivation for learning Topic Modeling?\n",
    "2. What does Topic Modeling actually do?\n",
    "3. How can Topic Modeling be used in Social Science research? \n",
    "4. <s> How do I get to the point where I can trust and interpret the model?</s>\n",
    "5. <s> How can I most efficiently access the tool?</s>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. What is the motivation for learning Topic Modeling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Themeatic Analysis** seeks to \"... identify **patterns** of meaning across a dataset that provide an answer to the research question being addressed.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![LDA Example](theme.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![LDA Example](urban-theme.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![LDA Example](big_data.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. What does Topic Modeling actually do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### LDA - Latent Dirichlet Allocation - Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "By using word co-occurence, LDA Topic Modeling identifies latent, major \"topics\" in a corpus (or a collection of documetns) and reports the words associated with the topic and categorizes the documents into these topics. \n",
    "\n",
    "**Example using 20,000 News Articles**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![LDA Example](Topic_Process.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Documents\n",
    "\n",
    "Documents are the individual texts that create a \"corpus.\"\n",
    "\n",
    "For example, in our newspaper example each document is a newspaper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Topics \n",
    "\n",
    "Topics are groups of words with assigned weights that denote their significance in the topic. These topics are selected based off of co-occurence of words. Random subset of topics presented below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Example of Related Technical Instruction](newspaper.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Example of Related Technical Instruction](keywords.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![LDA Example](probability.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Documents are given topic \"scores\". For example, document 1 could be 50% topic 1 and 50% topic 2. Since we can interpret topics as probability distributions over words, this means that 50% of the words from document 1 are \"drawn\" from topic 1 and 50% of words are drawn from topic 2. \n",
    "\n",
    "If document 1 has 100 words, 50 will be drawn from 1. If we use the distribution above as topic 1, the expected count of the word car is 35 (.7*50).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Upon getting the distance from each topic, we can then create a matrix of distance: \n",
    "\n",
    "$$\\mathbf{X} = \\left[\\begin{array}\n",
    "{rrr}\n",
    "0 & 1 & 2 \\\\\n",
    "1 & 0 & 3 \\\\\n",
    "2 & 3 & 0\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "From there we can do a variety of Multi-dimensional scaling (could do Metric MDS) to plot the Principal Components: \n",
    "\n",
    "$$d_{ij} = \\sqrt{\\sum_{k=1}^p (X_{ik} - X_{ik}})^2$$\n",
    "\n",
    "$$A = -\\frac{1}{2}d_{ij}^2 $$\n",
    "\n",
    "# ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alpho\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\socks.py:58: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Callable\n",
      "C:\\Users\\alpho\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\alpho\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\sparse\\sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "infile = open('lda_model_np','rb')\n",
    "lda_model = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "infile = open('corpus_np','rb')\n",
    "corpus = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "\n",
    "infile = open('id2word_np','rb')\n",
    "id2word = pickle.load(infile)\n",
    "infile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Community College\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![LDA Example](gp.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. How can Topic Modeling be used in Social Science research?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## NSF Project on STEM Apprenticeship : An Application of Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Overview of Project\n",
    "\n",
    "The STEM Apprenticeship project is an NSF funded project looking to examine the effectiveness of STEM Apprenticeships. One part of STEM Appreteinceship we care a lot about is examining the effectiveness of training people for sub-baculerate STEM careers (essentially, careers that require less training than a college degree but more than a high school degree). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We know STEM Appreteinceships and community colleges both train professionals for these role so we want to compare their effectivenss. Essentially our research question is **What skills differentiate community college STEM training and STEM Appreteinceships?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Example of Related Technical Instruction](RTI_Example.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Example of On the Job Training](OJT_Example.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Example of On the Job Training](example_c.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Example of On the Job Training](course_catalog.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Example of On the Job Training](course_catalog_d.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Example of On the Job Training](RTI_Comp.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### LDA on Community College course descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![LDA Example](LDA_CC.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![LDA Example](cc_topics.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![LDA Example](wp_topics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Work Process - Topics\n",
    "![LDA Example](results_wp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Community College - Topics\n",
    "![LDA Example](results_cc.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "path_link = \"C:/Users/alpho/Dropbox/NLP - STEM\"\n",
    "os.chdir(path_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "infile = open('lda_model_cc','rb')\n",
    "lda_model_cc = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "infile = open('corpus_cc','rb')\n",
    "corpus_cc = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "\n",
    "infile = open('id2word_cc','rb')\n",
    "id2word_cc = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "infile = open('lda_model_wp','rb')\n",
    "lda_model_wp = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "infile = open('corpus_wp','rb')\n",
    "corpus_wp = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "\n",
    "infile = open('id2word_wp','rb')\n",
    "id2word_wp = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Community College\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model_cc, corpus_cc, id2word_cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Community College\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Work Process\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model_wp, corpus_wp, id2word_wp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Work Process Schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4. How do I get to the point where I can trust and interpret the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5. How can I most efficiently access the tool?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Implementation of LDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "### Here we are going to import packages for our analysis\n",
    "import nltk; nltk.download('stopwords')\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "import itertools\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "import os\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "import pandas as pd\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import nltk.stem as stemmer\n",
    "from pprint import pprint\n",
    "import itertools\n",
    "import requests\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import os\n",
    "import collections\n",
    "import smart_open\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import gensim.models.doc2vec\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import nltk.tokenize \n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from string import digits\n",
    "from collections import namedtuple\n",
    "import math\n",
    "import matplotlib \n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pickle\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.spatial import distance\n",
    "from itertools import chain\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import sys\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import csv\n",
    "from sklearn.externals import joblib\n",
    "from string import digits\n",
    "import bz2\n",
    "from collections import Counter\n",
    "import plotly\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "plotly.tools.set_credentials_file(username='alphonse.m.simon', api_key='tL4FDjpWNTU1xUf5sXEl')\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "stemmer = PorterStemmer()\n",
    "np.random.seed(2018)\n",
    "nltk.download('wordnet')\n",
    "import xlsxwriter\n",
    "from xlrd import open_workbook\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "path_link = \"C:/Users/alpho/Dropbox/NLP - STEM\"\n",
    "os.chdir(path_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "work_process = pd.read_excel(open('./Data/NLP_STEM Data_final.xlsx','rb'), sheet_name=0)\n",
    "community_college = pd.read_excel(open('./Data/NLP_STEM Data_final.xlsx','rb'), sheet_name=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "community_college['clean_texts'] = community_college['Text'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "data_words = community_college['clean_texts'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "stopwords = ['student', 'cours', 'includ', 'topic', 'network', 'system', 'concept', 'fundament', 'skill', 'learn', \n",
    "            'complet', 'introduc', 'inform', 'cover', 'abil', 'abl', 'apprentic']\n",
    "for i in range(len(data_words)): \n",
    "    for x in stopwords: \n",
    "        try: \n",
    "            data_words[i][:] = [z for z in data_words[i] if z != x]\n",
    "        except ValueError: \n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words_bigrams)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_words_bigrams\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=8, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pprint(lda_model.print_topics())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 6. What other tools are avaiable in natural language processing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Word Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models\n",
    "import pandas as pd\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import nltk.stem as stemmer\n",
    "from pprint import pprint\n",
    "import itertools\n",
    "import requests\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import os\n",
    "import collections\n",
    "import smart_open\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import gensim.models.doc2vec\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import nltk.tokenize \n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from string import digits\n",
    "from collections import namedtuple\n",
    "import math\n",
    "import matplotlib \n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pickle\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.spatial import distance\n",
    "from itertools import chain\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import sys\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import csv\n",
    "from sklearn.externals import joblib\n",
    "from string import digits\n",
    "import bz2\n",
    "from collections import Counter\n",
    "import plotly\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "plotly.tools.set_credentials_file(username='alphonse.m.simon', api_key='tL4FDjpWNTU1xUf5sXEl')\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "stemmer = PorterStemmer()\n",
    "np.random.seed(2018)\n",
    "nltk.download('wordnet')\n",
    "\n",
    "path_link = \"C:/Users/alpho/Dropbox/NLP - Urban\"\n",
    "os.chdir(path_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "centers = tuple([ (\"https://www.urban.org/urban-wire/policy-center/center-international-development-and-governance\",\"IDG\") , \n",
    "            (\"https://www.urban.org/urban-wire/policy-center/center-labor-human-services-and-population\", \"LHP\"), \n",
    "            (\"https://www.urban.org/urban-wire/policy-center/center-nonprofits-and-philanthropy\", \"CNP\"), \n",
    "            (\"https://www.urban.org/urban-wire/policy-center/health-policy-center\", \"HPC\"), \n",
    "            (\"https://www.urban.org/urban-wire/policy-center/housing-finance-policy-center\", \"HFPC\"), \n",
    "            (\"https://www.urban.org/urban-wire/policy-center/income-and-benefits-policy-center\", \"IBP\"), \n",
    "            (\"https://www.urban.org/urban-wire/policy-center/justice-policy-center\", \"JPC\"), \n",
    "            (\"https://www.urban.org/urban-wire/policy-center/metropolitan-housing-and-communities-policy-center\", \"METRO\"),\n",
    "            (\"https://www.urban.org/urban-wire/policy-center/research-action-lab\", \"RAL\"), \n",
    "            (\"https://www.urban.org/urban-wire/policy-center/urban-brookings-tax-policy-center\", \"TPC\")])\n",
    "\n",
    "df = pd.DataFrame(columns=['Center','Url', 'Texts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def b_scrape(url_link, center):\n",
    "    global df\n",
    "    url_list = []\n",
    "    blog_posts = []\n",
    "    urban = \"https://www.urban.org\"\n",
    "    for i in range(1,100):\n",
    "        url_list.append(url_link + \"?page=\" + str(i))\n",
    "    for x in url_list: \n",
    "        page = requests.get(x)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        for a in soup.find_all('a', href=True): \n",
    "            temp = a['href']\n",
    "            if temp[0:12] == \"/urban-wire/\": \n",
    "                blog_posts.append(urban + a['href'])\n",
    "            else: \n",
    "                continue\n",
    "    blog_posts = list(set(blog_posts))\n",
    "    for x in blog_posts: \n",
    "        if any( [x[0:39] == \"https://www.urban.org/urban-wire/topic/\", x[0:37] == \"https://www.urban.org/urban-wire/rss?\"] ):\n",
    "             blog_posts.remove(x) \n",
    "        else: \n",
    "            continue\n",
    "    for x in blog_posts:\n",
    "        try: \n",
    "            page = requests.get(x)\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "            body_text = \"\"\n",
    "            for y in soup.find_all('p'):\n",
    "                body_text = body_text + \" \" + y.text\n",
    "            for strong_tag in soup.find_all('strong'):\n",
    "                body_text = body_text + \" \" + strong_tag.text + \" \" + str(strong_tag.next_sibling.string)\n",
    "            df2 = pd.DataFrame([[center, x, body_text]], columns=['Center','Url', 'Texts'])\n",
    "            df = df.append(df2)\n",
    "        except AttributeError:\n",
    "            continue\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "for x in centers: \n",
    "    b_scrape(x[0], x[1])\n",
    "\n",
    "df.to_pickle(\"./Text.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"./Text.pkl\")\n",
    "df = df.reset_index()\n",
    "remove = \"Your support helps Urban scholars continue to deliver evidence that can elevate debate, transform communities, and improve lives.\"\n",
    "also_remove = \"SIGN UP FOR OUR NEWSLETTERS\"\n",
    "\n",
    "\n",
    "df['Texts'] = df['Texts'].str.replace(remove, \"\", regex=True)\n",
    "df['Texts'] = df['Texts'].str.replace(also_remove, \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "df['clean_texts'] = df['Texts'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "training_set = [[]]\n",
    "\n",
    "for x in df['clean_texts']:\n",
    "    training_set.append(x)\n",
    "analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')\n",
    "docs = []\n",
    "for i, text in enumerate(training_set):\n",
    "    tags = [i]\n",
    "    docs.append(analyzedDocument(text, tags))\n",
    "\n",
    "# train word2vec model using gensim\n",
    "cores = multiprocessing.cpu_count()\n",
    "model = gensim.models.doc2vec.Doc2Vec(size=100, min_count=1, iter=200)\n",
    "\n",
    "\n",
    "model.build_vocab(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model_buyer = model.train(docs, total_examples=model.corpus_count, epochs=model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "fname = get_tmpfile(\"./my_doc2vec_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "model.save(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec.load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "cluster  = KMeans(n_clusters = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def get_element(element, position): \n",
    "    vector = model.infer_vector(element)\n",
    "    vector_element = vector[position]\n",
    "    return vector_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "v_list = []\n",
    "for i in range(0,101):\n",
    "    z = i - 1\n",
    "    df[\"v_\" + str(i)] = df[\"clean_texts\"].apply(get_element, position=z)\n",
    "    v_list.append(\"v_\" + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "df['cluster'] = cluster.fit_predict(df[df.columns[5:106]])\n",
    "vectors = df.columns[1:106]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "centroids = cluster.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Separating out the features\n",
    "from sklearn.decomposition import PCA\n",
    "x = df.loc[:, v_list].values\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "finalDf = pd.concat([principalDf, df], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "ax1 = finalDf.plot.scatter(x='principal component 1',\n",
    "                       y='principal component 2',\n",
    "                       c='DarkBlue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Much more detailed plot with word labels\n",
    "# Create random data with numpy\n",
    "c= ['hsl('+str(h)+',50%'+',50%)' for h in np.linspace(0, 360)]\n",
    "# Create a trace\n",
    "trace = go.Scatter(\n",
    "    x = finalDf['principal component 1'],\n",
    "    y = finalDf['principal component 2'],\n",
    "    mode = 'markers', \n",
    "    marker= dict(size= 14,\n",
    "                 line= dict(width=1),\n",
    "                 opacity= 0.3\n",
    "                ),\n",
    "        text= finalDf['Center'])\n",
    "\n",
    "\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "# Plot and embed in ipython notebook!\n",
    "py.iplot(data, filename ='basic-scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "finalDf['v_list'] = finalDf[v_list].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def EU(vector_list, centroid_index): \n",
    "    a = np.asarray(vector_list)\n",
    "    b = np.asarray(centroids[centroid_index])\n",
    "    cluster_distance = np.linalg.norm(a-b)\n",
    "    return cluster_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "top_cluster = []\n",
    "for x in finalDf['cluster'].unique():\n",
    "    finalDf['cluster_distance'] = finalDf['v_list'].apply(EU, centroid_index = df['cluster'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "finalDf.to_pickle(\"./Doc2Vec.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "finalDf = pd.read_pickle(\"./Doc2Vec.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def max_cluster(cluster):\n",
    "    df_temp = finalDf.loc[df['cluster'] == cluster]\n",
    "    cluster_list = df_temp.sort_values('cluster_distance', ascending=True)\n",
    "    cluster_list = df_temp['Center'][0:11].tolist()\n",
    "    return cluster_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "obj = {}\n",
    "for i in range(0,10):\n",
    "    max_cluster(i)\n",
    "    obj[\"Cluster \" + str(i + 1)] = max_cluster(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "table = pd.DataFrame.from_dict(obj, orient='index')\n",
    "latex_code = table.to_latex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "obj = {}\n",
    "\n",
    "for x in finalDf['Center'].unique():\n",
    "    obj[str(x)] = finalDf.loc[finalDf['Center'] == str(x), ['Texts']]\n",
    "for x in finalDf['Center'].unique():\n",
    "    obj[str(x)] = obj[str(x)]['Texts'].tolist()\n",
    "    obj[str(x)] = \" \".join(obj[str(x)])\n",
    "\n",
    "all_list = finalDf['Center'].unique()\n",
    "dictionary_list = []\n",
    "for x in all_list: \n",
    "    dictionary_list.append(obj[str(x)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(dictionary_list)\n",
    "cs_title = squareform(pdist(X.toarray(), 'cosine'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~alphonse.m.simon/2.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "trace = go.Heatmap(z=cs_title,\n",
    "                   x=all_list,\n",
    "                   y=all_list)\n",
    "data=[trace]\n",
    "py.iplot(data, filename='labelled-heatmap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
